
<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Wenzhao Zheng</title>
  
  <meta name="author" content="Wenzhao Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">  
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wenzhao Zheng</name>
              </p>
              <p> 
                I am currently a postdoctoral fellow in the Department of EECS at University of California, Berkeley, affiliated with <a href="http://bair.berkeley.edu/"> Berkeley Artificial Intelligence Research Lab (BAIR) </a> and <a href="https://deepdrive.berkeley.edu/"> Berkeley Deep Drive (BDD) </a>, supervised by Prof. <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>.
                Prior to that, I received my Ph.D degree from the Department of Automation at Tsinghua University, advised by Prof. <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a> and Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>. 
                In 2018, I received my BS degree from the Department of Physics, Tsinghua University. 
              </p>
              <p>
              I am generally interested in artificial intelligence and deep learning. My current research focuses on:
              </p>
              <p>ðŸ¦™ <b style="color:brown">Foudation Models</b> + ðŸš™ <b style="color:green">Physical Intelligence</b> ->  ðŸ¤– <b style="color:orange">AGI</b> </p>
              <li style="margin: 5px;">
                ðŸ¦™ <b style="color:brown">Foudation Models</b>: Large Models, Visual Generation, AI Safety...
              </li>
              <li style="margin: 5px;">
                ðŸš™ <b style="color:green">Physical Intelligence</b>: Spatial Understanding, Autonomous Driving, Embodied Robots...
              </li>
              </p>
              <b>If you want to work with me (in person or remotely) as an intern at BAIR, feel free to drop me an email at wzzheng@berkeley.edu.</b>
              <p style="text-align:center">
                <a href="mailto:wzzheng@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="files/CV_WenzhaoZheng.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=LdK9scgAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/wzzheng"> GitHub </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:80%;max-width:100%" alt="profile photo" src="images/wenzhaozheng.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2026-01:</b> 4 papers are accepted to <a href="https://iclr.cc/Conferences/2026">ICLR 2026</a>: 
                <a href="https://wzzheng.net/StreamVGGT" style="color:green">StreamVGGT</a>,
                <a href="https://howlin-wang.github.io/svg/" style="color:brown;">SVG</a>,
                <a href="https://eternalevan.github.io/Astra-project/" style="color:brown">Astra</a>,
                and
                <a href="https://arxiv.org/abs/2506.05473" style="color:green;">S2GO</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-09:</b> 3 papers are accepted to <a href="https://nips.cc/Conferences/2025">NeurIPS 2025</a>:
                <a href="https://ykiwu.github.io/Point3R/" style="color:green">Point3R</a>,
                <a href="https://github.com/zuosc19/QuadricFormer" style="color:green">QuadricFormer</a>,
                and
                <a href="https://github.com/EnVision-Research/DriveRecon" style="color:green;">DrivingRecon</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-06:</b> 6 papers are accepted to <a href="https://iccv.thecvf.com/">ICCV 2025</a>: 
                <a href="hhttps://ykiwu.github.io/EmbodiedOcc/" style="color:green;">EmbodiedOcc</a>,
                <a href="https://wzzheng.net/Stag" style="color:green">Stag-1</a>,
                <a href="https://huang-yh.github.io/spectralar/" style="color:brown">SpectralAR</a>,
                <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_PlaneRAS_Learning_Planar_Primitives_for_3D_Plane_Recovery_ICCV_2025_paper.pdf" style="color:green">PlaneRAS</a>,
                <a href="https://github.com/Zhangyr2022/D3QE" style="color:brown;">D<sup>3</sup>QE</a>,
                and
                <a href="https://github.com/yzheng97/CDAL" style="color:brown">CDAL</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-05:</b> 1 paper is accepted to <a href="https://icml.cc/Conferences/2025">ICML 2025</a>:
                <a href="https://github.com/Gumpest/SparseVLMs" style="color:brown">SparseVLM</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-02:</b> 4 papers are accepted to <a href="https://cvpr.thecvf.com/">CVPR 2025</a>: 
                <a href="https://motion-seg.github.io/" style="color:brown">SegAnyMo</a>,
                <a href="https://github.com/zuosc19/GaussianWorld" style="color:green">GaussianWorld</a>,
                <a href="https://github.com/huang-yh/GaussianFormer" style="color:green">GaussianFormer-2</a>,
                and
                <a href="https://github.com/chengweialan/DeSiRe-GS" style="color:green">DeSiRe-GS</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-01:</b> 1 paper is accepted to <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a>:
                <a href="https://wzzheng.net/UniDrive/" style="color:green">UniDrive</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-07:</b> 4 papers are accepted to <a href="https://eccv.ecva.net/Conferences/2024">ECCV 2024</a>: 
                <a href="https://wzzheng.net/OccWorld/" style="color:green">OccWorld</a>,
                <a href="https://github.com/wzzheng/GenAD" style="color:green">GenAD</a>,
                <a href="https://github.com/huang-yh/GaussianFormer" style="color:green">GaussianFormer</a>,
                and
                <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02019.pdf" style="color:brown">SpatialFormer</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-02:</b> 2 papers are accepted to <a href="https://cvpr2024.thecvf.com/">CVPR 2024</a>:
                <a href="https://huang-yh.github.io/SelfOcc/" style="color:green">SelfOcc</a>
                and
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_LowRankOcc_Tensor_Decomposition_and_Low-Rank_Recovery_for_Vision-based_3D_Semantic_CVPR_2024_paper.pdf" style="color:green">LowRankOcc</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-01:</b> 1 paper is accepted to <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a>:
                <a href="https://github.com/zbr17/SAMP" style="color:brown">SAMP</a>.
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
      <td style="text-indent:20px;width:100%;vertical-align:middle">
      <p>
        *Equal contribution &nbsp;&nbsp; <sup>â€ </sup>Project leader/Corresponding author.
      </p>
      </td>
    </tbody></table>

      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="text-indent:20px;width:100%;vertical-align:middle">
          <p><heading>Newest Papers</heading></p>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>





    </tbody></table> -->



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="text-indent:20px;width:100%;vertical-align:middle">
              <p><heading>Selected Papers <a href="https://scholar.google.com/citations?hl=zh-CN&user=LdK9scgAAAAJ&view_op=list_works&sortby=pubdate"> [Full List] </a></heading></p>
              <!-- <p>
                *Equal contribution &nbsp;&nbsp; <sup>â€ </sup>Project leader/Corresponding author.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          

          <!-- <h3 style="text-indent:20px;color:brown">Large Models</h3>:  -->
          <h3 style="text-indent:20px;color:brown">Large Models:
            <span style="color:black;">Native Multimodal Models, Efficient LLMs, Large Action Models...</span>
          </h3>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SparseVLM.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference</papertitle>
              <br> 
              <a href="https://gumpest.github.io/">Yuan Zhang*</a>,  
              <a href="https://scholar.google.com/citations?user=TxeAbWkAAAAJ">Chun-Kai Fan*</a>,  
              <a href="">Junpeng Ma*</a>,  
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href="https://taohuang.info/">Tao Huang</a>,  
              <a href="https://cfcs.pku.edu.cn/people/faculty/kuancheng/index.htm">Kuan Cheng</a>,  
              <a href="https://gudovskiy.github.io/">Denis Gudovskiy</a>,  
              <a href="">Tomoyuki Okuno</a>,  
              <a href="">Yohei Nakata</a>,  
              <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer</a>,
              <a href="https://www.shanghangzhang.com/"> Shanghang Zhang</a>
              <br>
              <em>International Conference on Machine Learning (<strong>ICML</strong></em>), 2025.
              <br>
              <a href="https://arxiv.org/abs/2410.04417">[arXiv]</a> 
              <a href="https://github.com/Gumpest/SparseVLMs">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=Gumpest&repo=SparseVLMs&type=star&count=true" >
              </iframe>
              <a href="https://leofan90.github.io/SparseVLMs.github.io/">[Project Page]</a>
              <!-- <a href="https://www.jiqizhixin.com/articles/2025-01-15">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
              <br>
              <p>SparseVLM sparsifies visual tokens adaptively based on the question prompt.</p>
            </td>
          </tr>


                    <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SegAnyMo.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Segment Any Motion in Videos</papertitle>
              <br>  
              <a href="https://github.com/nnanhuang">Nan Huang</a>,
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://www.chenfengx.com/">Chenfeng Xu </a>, 
              <a href="https://people.eecs.berkeley.edu/~keutzer/">Kurt Keutzer </a>, 
              <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>,
              <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
              <a href="https://qianqianwang68.github.io/">Qianqian Wang</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2503.22268">[arXiv]</a> 
              <a href="https://github.com/nnanhuang/SegAnyMo">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=nnanhuang&repo=SegAnyMo&type=star&count=true" >
              </iframe>
              <a href="https://motion-seg.github.io/">[Project Page]</a>
              <!-- <a href="https://zhuanlan.zhihu.com/p/17968801773">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
              <br>
              <p> Our model produces instance-level fine-grained moving object masks and can handle challenging scenarios including articulated structures, shadow reflections, dynamic background motion, and drastic camera movement.</p>
            </td>
          </tr>




        </tbody></table>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:brown">Visual Generation:
            <span style="color:black;">Generative Frameworks, World Models, Digital Humans...</span>
          </h3>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SVG.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SVG: Latent Diffusion Model without Variational Autoencoder</papertitle>
              <br> 
              <a href="https://github.com/shiml20">Minglei Shi*</a>, 
              <a href="https://howlin-wang.github.io/">Haolin Wang*</a>, 
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.ru/citations?user=fWxWEzsAAAAJ&hl=en">Ziyang Yuan</a>,  
              <a href="https://scholar.google.com/citations?user=cnOAMbUAAAAJ&hl=en">Xiaoshi Wu</a>,  
              <a href="https://xinntao.github.io/">Xintao Wang</a>,  
              <a href="https://scholar.google.com/citations?user=P6MraaYAAAAJ&hl=en"> Pengfei Wan</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2026.
              <br>
              <a href="https://arxiv.org/abs/2510.15301">[arXiv]</a> 
              <a href="https://github.com/shiml20/SVG">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=shiml20&repo=SVG&type=star&count=true">
              </iframe>
              <a href="https://huggingface.co/howlin/SVG/">[Model]</a>
              <a href="https://howlin-wang.github.io/svg/">[Project Page]</a>
              <a href="https://www.qbitai.com/2025/10/346706.html">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> SVG is a latent diffusion model without variational autoencoders to unleash Self-supervised representations for Visual Generation.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Astra.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Astra: General Interactive World Model with Autoregressive Denoising</papertitle>
              <br> 
              <a href="https://eternalevan.github.io/">Yixuan Zhu</a>, 
              <a href="https://github.com/Aurora-edu/">Jiaqi Feng</a>, 
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href="https://openreview.net/profile?id=~Yuan_Gao32">Yuan Gao</a>,  
              <a href="https://www.xtao.website/">Xin Tao</a>,  
              <a href="https://scholar.google.com/citations?user=P6MraaYAAAAJ&hl=en">Pengfei Wan</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2026.
              <br>
              <a href="https://arxiv.org/abs/2512.08931">[arXiv]</a> 
              <a href="https://github.com/EternalEvan/Astra">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=EternalEvan&repo=Astra&type=star&count=true">
              </iframe>
              <a href="https://huggingface.co/EvanEternal/Astra">[Model]</a>
              <a href="https://eternalevan.github.io/Astra-project/">[Project Page]</a>
              <br>
              <p> Astra is an interactive world model that delivers realistic long-horizon video rollouts under a wide range of scenarios and action inputs.  </p>
            </td>
          </tr>

        
        
        </tbody></table>
        


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:brown">AI Safety:
            <span style="color:black;">Visual Content Forensics, Transparent Models...</span>
          </h3>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/GenWorld.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>GenWorld: Towards Detecting AI-generated Real-world Simulation Videos</papertitle>
            <br> 
                <a href="https://chen-wl20.github.io/">Weiliang Chen</a>,
                <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
                <a href="https://yzheng97.github.io/">Yu Zheng</a>,
                <a href="https://leichenthu.github.io/">Lei Chen</a>, 
                <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
                <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
                <a href="https://duanyueqi.github.io/">Yueqi Duan</a>
            <br>
            <em><strong>arXiv</strong></em>, 2025.
            <br>
            <a href="https://arxiv.org/abs/2506.10975">[arXiv]</a> 
            <a href="https://github.com/chen-wl20/GenWorld">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=chen-wl20&repo=GenWorld&type=star&count=true" >
            </iframe>
            <a href="https://chen-wl20.github.io/GenWorld/">[Project Page]</a>
            <!-- <a href="https://mp.weixin.qq.com/s?__biz=MzU4NjQ1NzQyNQ==&mid=2247486813&idx=1&sn=a17268e992e758b6b163c2d94eed6cb5&chksm=fcd6a19ab897918046a1f0b997d312e8a8d01ad9aafa38615ecefdf2f68cfb95a267b8f0ca25&mpshare=1&scene=1&srcid=0131JRQQdlxwyTP7HDo4d0BT&sharer_shareinfo=310108161e2f4bda7291cfbbff104f1d&sharer_shareinfo_first=6b27789183ea7efa584ca5ae176dfe55#rd">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
            <br>
            <p> GenWorld features three key characteristics: 1) Real-world Simulation, 2) High Quality, and 3) Cross-prompt Diversity, which can serve as a foundation for AI-generated video detection research with practical significance.</p>
          </td>
        </tr>

      
        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/Skyra.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</papertitle>
            <br> 
                <a href="https://joeleelyf.github.io/">Yifei Li</a>,
                <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
                <a href="https://scholar.google.com/citations?hl=en&user=A8fcSBcAAAAJ">Yanran Zhang</a>,
                <a href="">Runze Sun</a>,
                <a href="https://yzheng97.github.io/">Yu Zheng</a>,
                <a href="https://leichenthu.github.io/">Lei Chen</a>, 
                <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
                <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
            <em><strong>arXiv</strong></em>, 2025.
            <br>
            <a href="https://arxiv.org/abs/2512.15693">[arXiv]</a> 
            <a href="https://github.com/JoeLeelyf/Skyra">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=JoeLeelyf&repo=Skyra&type=star&count=true" >
            </iframe>
            <a href="https://huggingface.co/collections/JoeLeelyf/skyra/">[Model]</a>
            <a href="https://joeleelyf.github.io/Skyra/">[Project Page]</a>
            <br>
            <p> Skyra focuses on Grounded Artifact Reasoning to simultaneously perform Artifact Perception, Spatio-Temporal Grounding, and Explanatory Reasoning.</p>
          </td>
        </tr>


        </tbody></table>



      


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
      
        <h3 style="text-indent:20px;color:green">Spatial Understanding:
            <span style="color:black;">4D Reconstruction, Native 4D Generation...</span>
          </h3>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/StreamVGGT.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Streaming 4D Visual Geometry Transformer</papertitle>
            <br>  
              <a href="">Dong Zhuo*</a>, 
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="">Jiahe Guo</a>, 
              <a href="https://github.com/YkiWu">Yuqi Wu</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2026.
            <br>
            <a href="https://arxiv.org/abs/2507.11539">[arXiv]</a> 
            <a href="https://github.com/wzzheng/StreamVGGT">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=StreamVGGT&type=star&count=true" >
            </iframe>
            <a href="https://wzzheng.net/StreamVGGT/">[Project Page]</a>
            <!-- <a href="https://zhuanlan.zhihu.com/p/12867656844">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
            <br>
            <p> StreamVGGT employs temporal causal attention and leverages cached token memory to support efficient incremental on-the-fly reconstruction, enabling interative and real-time online applications.  </p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/Point3R.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory</papertitle>
            <br> 
              <a href="https://github.com/YkiWu">Yuqi Wu*</a>,  
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2025.
            <br>
            <a href="https://arxiv.org/abs/2507.02863">[arXiv]</a> 
            <a href="https://github.com/YkiWu/Point3R">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=YkiWu&repo=Point3R&type=star&count=true" >
            </iframe>
            <a href="https://ykiwu.github.io/Point3R/">[Project Page]</a>
            <!-- <a href="https://zhuanlan.zhihu.com/p/12867656844">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
            <br>
            <p> Point3R is an online framework for dense streaming 3D reconstruction using explicit spatial memory, which achieves competitive performance with low training costs.  </p>
          </td>
        </tr>





      </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          
          
          <h3 style="text-indent:20px;color:green">Autonomou Driving<a href="https://github.com/wzzheng/LDM"> [Page]:
            <span style="color:black;">3D Occupancy Prediction, End-to-End Driving, 4D Driving Simulation...</span></a>
          </h3>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/tpvformer.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang* </a>, 
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2302.07817">[arXiv]</a>
              <a href="https://github.com/wzzheng/TPVFormer">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="100px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=TPVFormer&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/TPVFormer/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/614984007">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> Given only surround-camera motorcycle RGB images barrier as inputs, our model (trained using trailer only sparse traffic cone LiDAR point supervision) can predict the semantic occupancy for all volumes in the 3D space. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/OccWorld.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving</papertitle>
              <br> 
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://github.com/chen-wl20">Weiliang Chen*</a>,  
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ">Yuanhui Huang</a>, 
              <a href="http://boruizhang.site/">Borui Zhang</a>, 
              <a href="https://duanyueqi.github.io/">Yueqi Duan</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong></em>), 2024.
              <br>
              <a href="https://arxiv.org/abs/2311.16038">[arXiv]</a> 
              <a href="https://github.com/wzzheng/OccWorld">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=OccWorld&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/OccWorld/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/669979822">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> OccWorld models the joint evolutions of 3D scenes and ego movements and paves the way for interpretable end-to-end large driving models.  </p>
            </td>
          </tr>

          

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GenAD.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GenAD: Generative End-to-End Autonomous Driving</papertitle>
              <br> 
              <strong>Wenzhao Zheng*</strong>, 
              <a href="https://github.com/songruiqi"> Ruiqi Song* </a>,  
              <a href="https://scholar.google.com/citations?user=jPvOqgYAAAAJ"> Xianda Guo* </a>, 
              <a href=""> Chenming Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=jzvXnkcAAAAJ"> Long Chen</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong></em>), 2024.
              <br>
              <a href="https://arxiv.org/abs/2402.11502">[arXiv]</a> 
              <a href="https://github.com/wzzheng/GenAD">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=GenAD&type=star&count=true" >
              </iframe>
              <a href="https://zhuanlan.zhihu.com/p/683302211">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> GenAD casts end-to-end autonomous driving as a generative modeling problem.  </p>
            </td>
          </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/Doe-1.gif" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Doe-1: Closed-Loop Autonomous Driving with Large World Model</papertitle>
            <br> 
            <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
            <a href="">Zetian Xia*</a>,  
            <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
            <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo</a>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
            <em><strong>arXiv</strong></em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2412.09627">[arXiv]</a> 
            <a href="https://github.com/wzzheng/Doe">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=Doe&type=star&count=true" >
            </iframe>
            <a href="https://wzzheng.net/Doe/">[Project Page]</a>
            <a href="https://zhuanlan.zhihu.com/p/12867656844">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
            <br>
            <p> Doe-1 is the first closed-loop autonomous driving model for unified perception, prediction, and planning.  </p>
          </td>
        </tr>
        


        </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:green">Embodied Robots:
            <span style="color:black;">Vision-Language-Action Models, Embodied Simulation...</span>
          </h3>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/EmbodiedOcc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding</papertitle>
              <br> 
              <a href="https://github.com/YkiWu">Yuqi Wu*</a>,  
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo</a>, 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2412.04380">[arXiv]</a> 
              <a href="https://github.com/YkiWu/EmbodiedOcc">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=YkiWu&repo=EmbodiedOcc&type=star&count=true" >
              </iframe>
              <a href="https://ykiwu.github.io/EmbodiedOcc/">[Project Page]</a>
              <!-- <a href="https://zhuanlan.zhihu.com/p/12867656844">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
              <br>
              <p> EmbodiedOcc formulates an embodied 3D occupancy prediction task and employs a Gaussian-based framework to accomplish it.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SwiftVLA.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead</papertitle>
              <br> 
              <a href="">Chaojun Ni*</a>,  
              <a href="">Cheng Chen* </a>, 
              <a href="">Xiaofeng Wang* </a>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra">Zheng Zhu* </a>, 
              <strong>Wenzhao Zheng</strong>, 
              <a href="">Boyuan Wang</a>,  
              <a href="">Tianrun Chen </a>, 
              <a href="">Guosheng Zhao </a>, 
              <a href="">Haoyun Li </a>, 
              <a href="">Zhehao Dong</a>,  
              <a href="">Qiang Zhang </a>, 
              <a href="">Yun Ye </a>, 
              <a href="">Yang Wang </a>, 
              <a href="">Guan Huang </a>, 
              <a href="">Wenjun Mei </a>
              <br>
              <em><strong>arXiv</strong></em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2512.00903">[arXiv]</a> 
              <a href="https://github.com/GigaAI-research/SwiftVLA">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=GigaAI-research&repo=SwiftVLA&type=star&count=true" >
              </iframe>
              <a href="https://swiftvla.github.io/">[Project Page]</a>
              <br>
              <p> SwiftVLA integrates 4D spatiotemporal information into a lightweight vision-language-action model at minimal costs. </p>
            </td>
          </tr>

          </tbody></table>


        
          


          <div id="click">
            <p align=right><a href="#Show-the-full-publication-list" style="padding:20px;" onclick="showStuff(this);">My Ph.D. Research Topic</a></p>
           </div>
           <script>
             function showStuff(txt) {
               document.getElementById("full").style.display = "block";
               document.getElementById("click").style.display = "none";
              //  document.getElementById("selected").style.display = "none";
             }
             </script>

      <div id="full">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="text-indent:20px;width:100%;vertical-align:middle">
            <p><heading>My Ph.D. Research Topic</heading></p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:black">Deep Metric Learning</h3>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/IDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Introspective Deep Metric Learning</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ"> Chengkun Wang* </a>,
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2205.04449">[arXiv]</a>
              <a href="https://github.com/wangck20/IDML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=IDML&type=star&count=true" >
              </iframe>
              <br>
              <p> We propose an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/dml-dc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Metric Learning with Adaptively Composite Dynamic Constraints</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2023.
              <br>
              <a href="https://cloud.tsinghua.edu.cn/f/04dfc89e8f684a0ab4a3/?dl=1">[PDF]</a>
              <!-- <a href="https://github.com/wzzheng">[Code] (coming soon)</a> -->
              <br>
              <p> This paper formulates deep metric learning under a unified framework and propose a dynamic constraint generator to produce adaptive composite constraints to train the metric towards good generalization. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/HDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Hardness-Aware Deep Metric Learning</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://apuaachen.github.io/Zhaodong-Chen/"> Zhaodong Chen </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019 <em>(<strong style="color:red;">oral</strong>)</em>.
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2021.
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Hardness-Aware_Deep_Metric_Learning_CVPR_2019_paper.pdf">[PDF]</a>
              <a href="https://ieeexplore.ieee.org/abstract/document/9035438">[PDF] (Journal version)</a>
              <a href="https://github.com/wzzheng/hdml">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=HDML&type=star&count=true" >
              </iframe>
              <br>
              <p> We perform linear interpolation on embeddings to adaptively manipulate their hardness levels and generate corresponding label-preserving synthetics for recycled training.</p>
            </td> 
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DAML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Adversarial Metric Learning</papertitle>
              <br>
              <a href="https://duanyueqi.github.io/">Yueqi Duan</a>,
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://xudonglinthu.github.io/"> Xudong Lin </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018 <em>(<strong style="color:red;">spotlight</strong>)</em>.
              <br>
              <a href="https://duanyueqi.github.io/">Yueqi Duan</a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>IEEE Transactions on Image Processing (<strong>T-IP</strong>, IF: 11.041)</strong></em>, 2020.
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.pdf">[PDF]</a>
              <a href="https://ieeexplore.ieee.org/abstract/document/8883191">[PDF] (Journal version)</a>
              <a href="https://github.com/anonymous1computervision/DAML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=anonymous1computervision&repo=DAML&type=star&count=true" >
              </iframe>
              <br>
              <p> We generate potential hard negatives adversarial to the learned metric as complements.</p>
            </td> 
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Structural Deep Metric Learning for Room Layout Estimation</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020.
              <br>
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630715.pdf">[PDF]</a>
              <br>
              <p> We are the first to apply deep metric learning to prediction tasks with structured labels.</p>
            </td> 
          </tr>

          
        
      </tbody></table>





      </div>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2024 Excellent Doctoral Dissertation of Chinese Association for Artificial Intelligence</li>
                <li style="margin: 5px;"> 2023 Tsinghua Excellent Doctoral Dissertation Award</li>
                <li style="margin: 5px;"> 2023 Beijing Outstanding Graduate</li>
                <li style="margin: 5px;"> 2023 Tsinghua Outstanding Graduate</li>
                <li style="margin: 5px;"> 2022 Xuancheng Scholarship</li>
                <li style="margin: 5px;"> 2021 National Scholarship  (highest scholarship given by the government of China)</li>
                <li style="margin: 5px;"> 2021 CVPR Outstanding Reviewer</li>
                <li style="margin: 5px;"> 2020 Changtong Scholarship  (highest scholarship in the Dept. of Automation)</li>
                <li style="margin: 5px;"> 2019 National Scholarship  (highest scholarship given by the government of China)</li>
                <li style="margin: 5px;"> 2017 Tung OOCL Scholarship</li>
                <li style="margin: 5px;"> 2016 German Scholarship</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / PC Member:</b> CVPR 2019-2026, ICCV 2019-2025, ECCV 2020-2026, NeurIPS 2023-2025, ICLR 2024-2025, ICML 2025, IJCAI 2020-2025, WACV 2020-2025, ICME 2019-2025, 
              </li>
              <li style="margin: 5px;"> 
                <b>Senior PC Member:</b> IJCAI 2021
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> T-PAMI, T-IP, T-MM, T-CSVT, T-NNLS, T-BIOM, T-IST, Pattern Recognition, Pattern Recognition Letters
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <!-- <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=3Xl5HqLv8wQcw477KsV8mPSQEnrm59hQ6peJ0jKbxdw&cl=ffffff&w=a"></script>
	  </div>         -->
	  <br>
	    &copy; Wenzhao Zheng | Last updated: Feb. 1, 2026.
</center></p>
</body>

</html>
