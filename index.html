
<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Wenzhao Zheng</title>
  
  <meta name="author" content="Wenzhao Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wenzhao Zheng</name>
              </p>
              <p> 
                I am currently a postdoctoral fellow in the Department of EECS at University of California, Berkeley, affiliated with <a href="http://bair.berkeley.edu/"> Berkeley Artificial Intelligence Research Lab (BAIR) </a> and <a href="https://deepdrive.berkeley.edu/"> Berkeley Deep Drive (BDD) </a>, supervised by Prof. <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>.
                Prior to that, I received my Ph.D degree from the Department of Automation at Tsinghua University, advised by Prof. <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a> and Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>. 
                In 2018, I received my BS degree from the Department of Physics, Tsinghua University. 
                <!-- I am interested in computer vision and deep learning. My current research focuses on representation learning, autonomous driving, and explainable AI. -->
              </p>
              <p>
              I am interested in computer vision and deep learning. My current research focuses on:
              <li style="margin: 5px;" >
                <b style="color:brown">Vision-centric autonomous driving</b> that efficiently perceives and predicts the complex 3D world based on images.
              </li>
                <li style="margin: 5px;" >
                  <b style="color:green">Omni-supervised representation learning</b> that exploits various types of supervision signals to learn discriminative and generalizable visual representations.
                </li>
                 <li style="margin: 5px;" >
                  <b style="color:orange">Explainable artificial intelligence</b> that builds comprehensible and trustworthy AI systems with high performance.
                </li>
              </p>
              <b>If you want to work with me (in person or remotely) as an intern at BAIR, feel free to drop me an email at wzzheng@berkeley.edu. I will support GPUs if we are a good fit.</b>
              <p style="text-align:center">
                <a href="mailto:wzzheng@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="files/CV_WenzhaoZheng.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=LdK9scgAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/wzzheng"> GitHub </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/wenzhaozheng.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2024-07:</b> Four papers are accepted to <a href="https://eccv.ecva.net/Conferences/2024">ECCV 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-05:</b> One paper on lane detection is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">T-IP</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-04:</b> One paper on 3D object detection is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">T-MM</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-02:</b> Two papers on 3D occupancy prediction are accepted to <a href="https://cvpr2024.thecvf.com/">CVPR 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-01:</b> One paper on explainable deep learning is accepted to <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-09:</b> One paper on deep metric learning is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">T-PAMI</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-09:</b> One paper on unsupervised indoor depth completion is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">T-CSVT</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-07:</b> Three papers on representation learning and 3D occpuacy prediction are accepted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-01:</b> Two papers on 3D occpuacy prediction and deep metric learning are accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-01:</b> One paper on explainable deep networks is accepted to <a href="https://iclr.cc/Conferences/2023">ICLR 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-01:</b> One paper on deep metric learning is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">T-PAMI</a>.
              </li>
              <!-- <li style="margin: 5px;" >
                <b>2022-12:</b> One paper on 3D object detection is accepted to <a href="https://aaai.org/Conferences/AAAI-23/">AAAI 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2022-09:</b> One paper on surrounding depth estimation is accepted to <a href="https://corl2022.org/">CoRL 2022</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2022-07:</b> One paper on dynamic metric learning is accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2022-03:</b> Two papers on explainable metric learning and 3D object detection are accepted to <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-07:</b> One paper on deep metric learning is accepted to <a href="http://iccv2021.thecvf.com/">ICCV 2021</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-02:</b> One paper on deep metric learning is accepted to <a href="https://cvpr2021.thecvf.com/">CVPR 2021</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2020-07:</b> One paper on room layout estimation is accepted to <a href="https://eccv2020.eu/">ECCV 2020</a>.
              </li> -->
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
      <td style="text-indent:20px;width:100%;vertical-align:middle">
      <p>
        *Equal contribution &nbsp;&nbsp; <sup>†</sup>Project leader/Corresponding author.
      </p>
      </td>
    </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="text-indent:20px;width:100%;vertical-align:middle">
          <p><heading>Newest Papers</heading></p>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="images/OccSora.gif" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving</papertitle>
          <br> 
          <a href="https://github.com/LeningWang"> Lening Wang* </a>, 
          <strong>Wenzhao Zheng*<sup>, †</sup></strong>, 
          <a href="https://shi.buaa.edu.cn/renyilong/zh_CN/index.htm"> Yilong Ren </a>, 
          <a href="https://scholar.google.com/citations?user=d0WJTQgAAAAJ&hl"> Han Jiang </a>, 
          <a href="https://zhiyongcui.com/"> Zhiyong Cui </a>, 
          <a href="https://shi.buaa.edu.cn/09558/zh_CN/index.htm"> Haiyang Yu </a>, 
          <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
          <br>
          <em><strong>arXiv</strong></em>, 2024.
          <br>
          <a href="https://arxiv.org/abs/2405.20337">[arXiv]</a> 
          <a href="https://github.com/wzzheng/OccSora">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=OccSora&type=star&count=true" >
          </iframe>
          <a href="https://wzzheng.net/OccSora/">[Project Page]</a>
          <br>
          <p> With trajectory-aware 4D generation, OccSora has the potential to serve as a world simulator for the decision-making of autonomous driving.  </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="images/S3Gaussian.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>S<sup>3</sup>Gaussian: Self-Supervised Street Gaussians for Autonomous Driving</papertitle>
          <br> 
          <a href="https://github.com/nnanhuang"> Nan Huang </a>, 
          <a href="https://ucwxb.github.io/"> Xiaobao Wei </a>, 
          <strong>Wenzhao Zheng<sup>†</sup></strong>, 
          <a href=""> Pengju An </a>, 
          <a href="https://lu-m13.github.io/"> Ming Lu </a>, 
          <a href="https://zhanwei.site/"> Wei Zhan </a>, 
          <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/"> Masayoshi Tomizuka </a>, 
          <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>, 
          <a href="https://www.shanghangzhang.com/"> Shanghang Zhang </a>
          <br>
          <em><strong>arXiv</strong></em>, 2024.
          <br>
          <a href="https://arxiv.org/abs/2405.20323">[arXiv]</a> 
          <a href="https://github.com/nnanhuang/S3Gaussian">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=nnanhuang&repo=S3Gaussian&type=star&count=true" >
          </iframe>
          <a href="https://wzzheng.net/S3Gaussian/">[Project Page]</a>
          <br>
          <p> S<sup>3</sup>Gaussian employs 3D Gaussians to model dynamic scenes for autonomous driving <b>without</b> other supervisions (e.g., 3D bounding boxes).  </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="images/GaussianFormer.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction</papertitle>
          <br> 
          <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
          <strong>Wenzhao Zheng<sup>†</sup></strong>, 
          <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang </a>, 
          <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
          <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
          <br>
          <em>European Conference on Computer Vision (<strong>ECCV</strong></em>), 2024.
          <br>
          <a href="https://arxiv.org/abs/2405.17429">[arXiv]</a> 
          <a href="https://github.com/huang-yh/GaussianFormer">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=huang-yh&repo=GaussianFormer&type=star&count=true" >
          </iframe>
          <a href="https://wzzheng.net/GaussianFormer/">[Project Page]</a>
          <a href="https://zhuanlan.zhihu.com/p/700833107">[中文解读 (in Chinese)]</a>
          <br>
          <p> GaussianFormer proposes the 3D semantic Gaussians as <b>a more efficient object-centric</b> representation for driving scenes compared with 3D occupancy.  </p>
        </td>
      </tr>
          
      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="images/HASS.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>Hardness-Aware Scene Synthesis for Semi-Supervised 3D Object Detection</papertitle>
          <br> 
          <a href="https://github.com/songruiqi"> Shuai Zeng </a>, 
          <strong>Wenzhao Zheng<sup>†</sup></strong>, 
          <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
          <a href="https://scholar.google.com/citations?user=-AQLKlsAAAAJ"> Haibin Yan </a>,
          <br>
          <em>IEEE Transactions on Multimedia (<strong>T-MM</strong>, IF: 7.3)</em>, 2024.
          <br>
          <a href="https://arxiv.org/abs/2405.17422">[arXiv]</a> 
          <a href="https://github.com/wzzheng/HASS">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=HASS&type=star&count=true" >
          </iframe>
          <br>
          <p> HASS proposes a scene synthesis strategy to adaptively generate challenging synthetic scenes for more generalizable semi-supervised 3D object detection.  </p>
        </td>
      </tr>


      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="images/GenAD.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>GenAD: Generative End-to-End Autonomous Driving</papertitle>
          <br> 
          <strong>Wenzhao Zheng*</strong>, 
          <a href="https://github.com/songruiqi"> Ruiqi Song* </a>,  
          <a href="https://scholar.google.com/citations?user=jPvOqgYAAAAJ"> Xianda Guo* </a>, 
          <a href=""> Chenming Zhang </a>, 
          <a href="https://scholar.google.com/citations?user=jzvXnkcAAAAJ"> Long Chen</a>
          <br>
          <em>European Conference on Computer Vision (<strong>ECCV</strong></em>), 2024.
          <br>
          <a href="https://arxiv.org/abs/2402.11502">[arXiv]</a> 
          <a href="https://github.com/wzzheng/GenAD">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=GenAD&type=star&count=true" >
          </iframe>
          <a href="https://zhuanlan.zhihu.com/p/683302211">[中文解读 (in Chinese)]</a>
          <br>
          <p> GenAD casts autonomous driving as a generative modeling problem.  </p>
        </td>
      </tr>
    </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="text-indent:20px;width:100%;vertical-align:middle">
              <p><heading>Selected Papers</heading></p>
              <!-- <p>
                *Equal contribution &nbsp;&nbsp; <sup>†</sup>Project leader/Corresponding author.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <h3 style="text-indent:20px;color:brown">Autonomous Driving</h3>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/OccWorld.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving</papertitle>
              <br> 
              <strong>Wenzhao Zheng*</strong>, 
              <a href="https://github.com/chen-wl20"> Weiliang Chen* </a>,  
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
              <a href="http://boruizhang.site/"> Borui Zhang </a>, 
              <a href="https://duanyueqi.github.io/"> Yueqi Duan</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong></em>), 2024.
              <br>
              <a href="https://arxiv.org/abs/2311.16038">[arXiv]</a> 
              <a href="https://github.com/wzzheng/OccWorld">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=OccWorld&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/OccWorld/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/669979822">[中文解读 (in Chinese)]</a>
              <br>
              <p> OccWorld models the joint evolutions of 3D scenes and ego movements and paves the way for interpretable end-to-end large driving models.  </p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SelfOcc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang* </a>, 
              <strong>Wenzhao Zheng*</strong>, 
              <a href="http://boruizhang.site/"> Borui Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2311.12754">[arXiv]</a>
              <a href="https://github.com/huang-yh/SelfOcc">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=huang-yh&repo=SelfOcc&type=star&count=true" >
              </iframe>
              <a href="https://huang-yh.github.io/SelfOcc/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/677380563">[中文解读 (in Chinese)]</a>
              <br>
              <p> SelfOcc is the first self-supervised work that produces reasonable 3D occupancy for surround cameras. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PointOcc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction</papertitle>
              <br> 
              <a href="https://github.com/zuosc19"> Sicheng Zuo* </a>, 
              <strong>Wenzhao Zheng*</strong>, 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em><strong>arXiv</strong></em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2308.16896">[arXiv]</a>
              <a href="https://github.com/wzzheng/PointOcc">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=PointOcc&type=star&count=true" >
              </iframe>
              <a href="https://zhuanlan.zhihu.com/p/668842814">[中文解读 (in Chinese)]</a>
              <br>
              <p> As the first 2D-projection-based method on the 3D semantic occupancy prediction task, PointOcc significantly outperforms all other methods by a large margin with a much faster speed. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/surroundocc.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving</papertitle>
              <br>
              <a href="https://weiyithu.github.io/"> Yi Wei*</a>, 
              <a href="https://github.com/lqzhao"> Linqing Zhao*</a>, 
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2303.09551">[arXiv]</a>
              <a href="https://github.com/weiyithu/SurroundOcc">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=weiyithu&repo=SurroundOcc&type=star&count=true" >
              </iframe>
              <a href="https://zhuanlan.zhihu.com/p/652974165">[中文解读 (in Chinese)]</a>
              <br>
              <p> We design a pipeline to generate dense occupancy ground truths without expensive occupancy annotations, which enalbes the training of more dense 3D occupancy prediction models. </p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/tpvformer.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang* </a>, 
              <strong>Wenzhao Zheng*</strong>, 
              <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2302.07817">[arXiv]</a>
              <a href="https://github.com/wzzheng/TPVFormer">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="100px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=TPVFormer&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/TPVFormer/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/614984007">[中文解读 (in Chinese)]</a>
              <br>
              <p> Given only surround-camera motorcycle RGB images barrier as inputs, our model (trained using trailer only sparse traffic cone LiDAR point supervision) can predict the semantic occupancy for all volumes in the 3D space. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/BEVerse.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>BEVerse: Unified Perception and Prediction in Birds-Eye-View for Vision-Centric Autonomous Driving</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://huangjunjie2017.github.io/"> Junjie Huang</a>, 
              Guan Huang,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em><strong>arXiv</strong></em>, 2022.
              <br>
              <a href="https://arxiv.org/abs/2205.09743">[arXiv]</a>
              <a href="https://github.com/zhangyp15/BEVerse">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=zhangyp15&repo=BEVerse&type=star&count=true" >
              </iframe>
              <a href="https://zhuanlan.zhihu.com/p/518147623">[中文解读 (in Chinese)]</a>
              <br>
              <p> We propose a unified framework for 3D perception and prediction based on multi-camera systems. The multi-task BEVerse outperforms existing single-task methods on 3D object detection, semantic map construction, and motion prediction. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/surrounddepth.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation</papertitle>
              <br> 
              <a href="https://weiyithu.github.io/"> Yi Wei*</a>, 
              <a href="https://github.com/lqzhao"> Linqing Zhao*</a>, 
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              <a href="https://raoyongming.github.io/"> Yonming Rao</a>, 
              Guan Huang,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>Conference on Robot Learning (<strong>CoRL</strong>)</em>, 2022.
              <br>
              <a href="https://arxiv.org/abs/2204.03636">[arXiv]</a>
              <a href="https://github.com/weiyithu/SurroundDepth">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=weiyithu&repo=SurroundDepth&type=star&count=true" >
              </iframe>
              <a href="https://zhuanlan.zhihu.com/p/521622046">[中文解读 (in Chinese)]</a>
              <br>
              <p> We propose a SurroundDepth method to incorporate the information from multiple surrounding views to predict depth maps across cameras. </p>
            </td>
          </tr>

          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:green">Representation Learning</h3>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/IDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Introspective Deep Metric Learning</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ"> Chengkun Wang* </a>,
              <strong>Wenzhao Zheng*</strong>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2205.04449">[arXiv]</a>
              <a href="https://github.com/wangck20/IDML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=IDML&type=star&count=true" >
              </iframe>
              <br>
              <p> We propose an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. </p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/OPERA.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ"> Chengkun Wang* </a>,
              <strong>Wenzhao Zheng*</strong>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2210.05557">[arXiv]</a>
              <a href="https://github.com/wangck20/OPERA">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wangck20&repo=OPERA&type=star&count=true" >
              </iframe>
              <br>
              <p> We unify fully supervised and self-supervised contrastive learning and exploit both supervisions from labeled and unlabeled data for training. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/TL-Align.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Token-Label Alignment for Vision Transformers</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=N-u2i-QAAAAJ&hl=zh-CN&oi=sra">  Han Xiao*</a>, 
              <strong>Wenzhao Zheng*</strong>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2210.06455">[arXiv]</a>
              <a href="https://github.com/Euphoria16/TL-Align">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=Euphoria16&repo=TL-Align&type=star&count=true" >
              </iframe>
              <br>
              <p> We identify a token fluctuation phenomenon that has suppressed the potential of data mixing strategies for vision transformers. To adress this, we propose a token-label alignment (TL-Align) method to trace the correspondence between transformed tokens and the original tokens to maintain a label for each token. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/dml-dc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Metric Learning with Adaptively Composite Dynamic Constraints</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2023.
              <br>
              <a href="https://cloud.tsinghua.edu.cn/f/04dfc89e8f684a0ab4a3/?dl=1">[PDF]</a>
              <!-- <a href="https://github.com/wzzheng">[Code] (coming soon)</a> -->
              <br>
              <p> This paper formulates deep metric learning under a unified framework and propose a dynamic constraint generator to produce adaptive composite constraints to train the metric towards good generalization. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/HDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Hardness-Aware Deep Metric Learning</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://apuaachen.github.io/Zhaodong-Chen/"> Zhaodong Chen </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019 <em>(<strong style="color:red;">oral</strong>)</em>.
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Hardness-Aware_Deep_Metric_Learning_CVPR_2019_paper.pdf">[PDF]</a>
              <a href="https://github.com/wzzheng/hdml">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=HDML&type=star&count=true" >
              </iframe>
              <br>
              <p> We perform linear interpolation on embeddings to adaptively manipulate their hardness levels and generate corresponding label-preserving synthetics for recycled training.</p>
            </td> 
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DAML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Adversarial Metric Learning</papertitle>
              <br>
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://xudonglinthu.github.io/"> Xudong Lin </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018 <em>(<strong style="color:red;">spotlight</strong>)</em>.
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.pdf">[PDF]</a>
              <a href="https://github.com/anonymous1computervision/DAML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=anonymous1computervision&repo=DAML&type=star&count=true" >
              </iframe>
              <br>
              <p> We generate potential hard negatives adversarial to the learned metric as complements.</p>
            </td> 
          </tr>
      

        </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <h3 style="text-indent:20px;color:orange">Explainable Artificial Intelligence</h3>


            <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SAMP.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Path Choice Matters for Clear Attribution in Path Methods</papertitle>
              <br>
              <a href="http://boruizhang.site/"> Borui Zhang</a>, 
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2401.10442">[arXiv]</a>
              <a href="https://github.com/zbr17/SAMP">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=zbr17&repo=SAMP&type=star&count=true" >
              </iframe>
              <br>
              <p> To address the ambiguity in attributions caused by different path choices, we introduced the Concentration Principle and developed SAMP, an efficient model-agnostic interpreter. By incorporating the infinitesimal constraint (IC) and momentum strategy (MS), SAMP provides superior interpretations.</p>
            </td>
          </tr>

            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/SimShap.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <papertitle>Exploring Unified Perspective For Fast Shapley Value Estimation</papertitle>
                <br>
                <a href="http://boruizhang.site/"> Borui Zhang*</a>, 
                <a href="https://tongclass.ac.cn/author/baotong-tian/">Baotong Tian*</b></a>, 
                <strong>Wenzhao Zheng</strong>, 
                <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&amp;hl=en&amp;authuser=1">Jie Zhou</a>, 
                <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
                <br>
                <em><strong>arXiv</strong></em>, 2023
                <!-- <em>arXiv (<strong>ICLR</strong>)</em>, 2023 -->
                <br>
                <a href="https://arxiv.org/abs/2311.01010">[arXiv]</a> 
                <a href="https://github.com/User-tian/SimSHAP">[Code]</a>
                <iframe
                style="margin-left: 2px; margin-bottom:-5px;"
                frameborder="0" scrolling="0" width="91px" height="20px"
                src="https://ghbtns.com/github-btn.html?user=User-tian&repo=SimSHAP&type=star&count=true" >
                </iframe>
                <p>
                  This paper analyzes the consistency of existing Shapley value estimators and proposes the simple amortized estimator, SimSHAP. 
                  Extensive experiments conducted on tabular and image datasets validate the effectiveness of our SimSHAP, which significantly accelerates the computation of accurate Shapley values.
                </p>
              </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/bort.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Bort: Towards Explainable Neural Networks with Bounded Orthogonal Constraint</papertitle>
              <br>
              <a href="http://boruizhang.site/"> Borui Zhang</a>, 
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2212.09062">[arXiv]</a>
              <a href="https://github.com/zbr17/Bort">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=zbr17&repo=Bort&type=star&count=true" >
              </iframe>
              <br>
              <p> This paper proposes Bort, an optimizer for improving model explainability with boundedness and orthogonality constraints on model parameters, derived from the sufficient conditions of model comprehensibility and transparency.</p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/AVSL.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Attributable Visual Similarity Learning</papertitle>
              <br>
              <a href="http://boruizhang.site/"> Borui Zhang</a>, 
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022.
              <br>
              <a href="https://arxiv.org/abs/2203.14932">[arXiv]</a>
              <a href="https://github.com/zbr17/DRML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=zbr17&repo=DRML&type=star&count=true" >
              </iframe>
              <br>
              <p> This paper proposes an attributable visual similarity learning (AVSL) framework, which employs a generalized similarity learning paradigm to represent the similarity between two images with a graph for a more accurate and explainable similarity measure between images.</p>
            </td> 
          </tr>



        </tbody></table>


          <div id="click">
            <p align=right><a href="#Show-the-full-publication-list" style="padding:20px;" onclick="showStuff(this);">Full publication list</a></p>
           </div>
           <script>
             function showStuff(txt) {
               document.getElementById("full").style.display = "block";
               document.getElementById("click").style.display = "none";
              //  document.getElementById("selected").style.display = "none";
             }
             </script>

      <div id="full">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="text-indent:20px;width:100%;vertical-align:middle">
            <p><heading>Other Papers</heading></p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <!-- <h3 style="text-indent:20px;">Other Works</h3> -->

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SPTR.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SPTR: Structure-Preserving Transformer for Unsupervised Indoor Depth Completion</papertitle>
              <br>
              <a href="https://github.com/lqzhao"> Linqing Zhao</a>, 
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://duanyueqi.github.io/"> Yueqi Duan</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>T-CSVT</strong>, IF: 8.4)</em>, 2023.
              <br>
              <a href="https://ieeexplore.ieee.org/document/10243117">[PDF]</a>
              <!-- <a href="https://github.com/wzzheng">[Code (coming soon)]</a> -->
              <br>
              <p> We propose a Structure-Preserving Encoding (SPE) module to reformulate depth completion as a process of 3D structure generation. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DFML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Factorized Metric Learning</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ"> Chengkun Wang* </a>, 
              <strong>Wenzhao Zheng*</strong>, 
              Junlong Li, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023.
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Factorized_Metric_Learning_CVPR_2023_paper.pdf">[PDF]</a>
              <!-- <a href="https://github.com/wzzheng">[Code (coming soon)]</a> -->
              <br>
              <p> We factorize the backbone network to different sub-blocks and learns an adaptive route for each sample to achieve the diversity of features.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Probabilistic Deep Metric Learning for Hyperspectral Image Classification</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ"> Chengkun Wang </a>,
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com.hk/citations?user=xWKPHDYAAAAJ&hl=zh-CN"> Xian Sun </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a> 
              <br>
              <em><strong>arXiv</strong></em>, 2022.
              <br>
              <a href="https://arxiv.org/abs/2211.08349">[arXiv]</a>
              <a href="https://github.com/wzzheng/PDML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=PDML&type=star&count=true" >
              </iframe>
              <br>
              <p> We propose a probabilistic deep metric learning framework to model the categorical uncertainty of the spectral distribution of an observed pixel for Hyperspectral image classification. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/CLCD.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Dynamic Metric Learning with Cross-Level Concept Distillation</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
              <a href="http://boruizhang.site/"> Borui Zhang</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2022.
              <br>
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136840194.pdf">[PDF]</a>
              <a href="https://github.com/wzzheng/CLCD">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=CLCD&type=star&count=true" >
              </iframe>
              <br>
              <p> This paper propose a hierarchical concept refiner to construct multiple levels of concept embeddings of an image and them pull closer the distance of the corresponding concepts to facilitate the cross-level semantic structure of the image representations. </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SimMOD.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>A Simple Baseline for Multi-Camera 3D Object Detection</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang </a>, 
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              Guan Huang,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>Thirty-Seventh AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2208.10035">[arXiv]</a>
              <a href="https://github.com/zhangyp15/SimMOD">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=zhangyp15&repo=SimMOD&type=star&count=true" >
              </iframe>
              <br>
              <p> We propose a simple baseline for multi-camera object detection to adapt existing monocular 3D object detection methods with a two-stage propose-and-fuse framework.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Dimension.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Dimension Embeddings for Monocular 3D Object Detection</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang </a>, 
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              Guan Huang,
              Dalong Du,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022.
              <br>
              <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Dimension_Embeddings_for_Monocular_3D_Object_Detection_CVPR_2022_paper.pdf">[PDF]</a>
              <br>
              <p> We propose a general method to learn appropriate embeddings for dimension estimation in monocular 3D object detection.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DRML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Relational Metric Learning</papertitle>
              <br>
              <strong>Wenzhao Zheng*</strong>, 
              <a href="http://boruizhang.site/"> Borui Zhang*</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021.
              <br>
              <a href="http://arxiv.org/abs/2108.10026">[arXiv]</a>
              <a href="https://github.com/zbr17/DRML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=zbr17&repo=DRML&type=star&count=true" >
              </iframe>
              <br>
              <p> We construct a graph to represent each image and perform relational inference to infer the visual similarity.</p>
            </td> 
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DCML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Compositional Metric Learning</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ"> Chengkun Wang </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021.
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Deep_Compositional_Metric_Learning_CVPR_2021_paper.pdf">[PDF]</a>
              <a href="https://github.com/wzzheng/dcml">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=dcml&type=star&count=true" >
              </iframe>
              <br>
              <p> We adaptively learn a set of composites of embeddings to receive supervision signals from different tasks to improve the generalization of the learned embeddings without sacriﬁcing the discriminativeness.</p>
            </td> 
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Structural Deep Metric Learning for Room Layout Estimation</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020.
              <br>
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630715.pdf">[PDF]</a>
              <br>
              <p> We are the first to apply deep metric learning to prediction tasks with structured labels.</p>
            </td> 
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DML-ALA.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Metric Learning via Adaptive Learnable Assessment</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020.
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Deep_Metric_Learning_via_Adaptive_Learnable_Assessment_CVPR_2020_paper.pdf">[PDF]</a>
              <br>
              <p> We learn a sample assessment strategy for deep metric learning to maximize the generalization of the trained metric.</p>
            </td> 
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/HDML_PAMI.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Hardness-Aware Deep Metric Learning</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2021.
              <br>
              <a href="https://cloud.tsinghua.edu.cn/f/6b71a06d277449c481a0/?dl=1">[PDF]</a>
              <a href="https://github.com/wzzheng/hdml">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=hdml&type=star&count=true" >
              </iframe>
              <br>
              <p> We extend the previous conference-verision HDML to generate multiple synthetics for each sample.</p>
            </td> 
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DAML-TIP.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Adversarial Metric Learning</papertitle>
              <br>
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE Transactions on Image Processing (<strong>T-IP</strong>, IF: 11.041)</strong></em>, 2020.
              <br>
              <a href="https://cloud.tsinghua.edu.cn/f/eb9f18e4721f474a93db/?dl=1">[PDF]</a>
              <a href="https://github.com/anonymous1computervision/DAML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=anonymous1computervision&repo=DAML&type=star&count=true" >
              </iframe>
              <br>
              <p> We propose a deep adversarial multi-metric learning (DAMML) method by learning multiple local transformations for more complete description.</p>
            </td> 
          </tr>


          
        
      </tbody></table>
      </div>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> Tsinghua Excellent Doctoral Dissertation Award</li>
                <li style="margin: 5px;"> 2023 Beijing Outstanding Graduate</li>
                <li style="margin: 5px;"> 2023 Tsinghua Outstanding Graduate</li>
                <li style="margin: 5px;"> 2022 Xuancheng Scholarship</li>
                <li style="margin: 5px;"> 2021 National Scholarship  (highest scholarship given by the government of China)</li>
                <li style="margin: 5px;"> CVPR 2021 Outstanding Reviewer</li>
                <li style="margin: 5px;"> 2020 Changtong Scholarship  (highest scholarship in the Dept. of Automation)</li>
                <li style="margin: 5px;"> 2019 National Scholarship  (highest scholarship given by the government of China)</li>
                <li style="margin: 5px;"> 2017 Tung OOCL Scholarship</li>
                <li style="margin: 5px;"> 2016 German Scholarship</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / PC Member:</b> CVPR 2019-2024, ICCV 2019-2023, ECCV 2020-2022, NeurIPS 2023, ICLR 2024, IJCAI 2020-2022, WACV 2020-2022, ICME 2019-2022, 
              </li>
              <li style="margin: 5px;"> 
                <b>Senior PC Member:</b> IJCAI 2021
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> T-PAMI, T-NNLS, T-IP, T-BIOM, T-IST, Pattern Recognition, Pattern Recognition Letters
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <!-- <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=3Xl5HqLv8wQcw477KsV8mPSQEnrm59hQ6peJ0jKbxdw&cl=ffffff&w=a"></script>
	  </div>         -->
	  <br>
	    &copy; Wenzhao Zheng | Last updated: June 1, 2024.
</center></p>
</body>

</html>
