
<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Wenzhao Zheng</title>
  
  <meta name="author" content="Wenzhao Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">  
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wenzhao Zheng</name>
              </p>
              <p> 
                I am currently a postdoctoral fellow in the Department of EECS at University of California, Berkeley, affiliated with <a href="http://bair.berkeley.edu/"> Berkeley Artificial Intelligence Research Lab (BAIR) </a> and <a href="https://deepdrive.berkeley.edu/"> Berkeley Deep Drive (BDD) </a>, supervised by Prof. <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>.
                Prior to that, I received my Ph.D degree from the Department of Automation at Tsinghua University, advised by Prof. <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a> and Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>. 
                In 2018, I received my BS degree from the Department of Physics, Tsinghua University. 
              </p>
              <p>
              I am generally interested in artificial intelligence and deep learning. My current research focuses on:
              </p>
              <p>ðŸ¦™ <b style="color:brown">Large Models</b> + ðŸš™ <b style="color:green">Embodied Agents</b> ->  ðŸ¤– <b style="color:orange">AGI</b> </p>
              <li style="margin: 5px;">
                ðŸ¦™ <b style="color:brown">Large Models and World Models</b>: Efficient/Small LLMs, Multimodal Models, Video Generation Models, Large Action Models...
              </li>
              <li style="margin: 5px;">
                ðŸš™ <b style="color:green">Embodied Agents and Spatial Intelligence</b>: 3D Occupancy Prediction, End-to-End Driving, 3D Scene Reconstruction, 4D Scene Simulation...
              </li>
              </p>
              <b>If you want to work with me (in person or remotely) as an intern at BAIR, feel free to drop me an email at wzzheng@berkeley.edu.</b>
              <p style="text-align:center">
                <a href="mailto:wzzheng@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="files/CV_WenzhaoZheng.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=LdK9scgAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/wzzheng"> GitHub </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/wenzhaozheng.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2026-01:</b> 4 papers are accepted to <a href="https://iclr.cc/Conferences/2026">ICLR 2026</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-09:</b> 3 papers are accepted to <a href="https://nips.cc/Conferences/2025">NeurIPS 2025</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-06:</b> 6 papers are accepted to <a href="https://iccv.thecvf.com/">ICCV 2025</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-05:</b> 1 paper is accepted to <a href="https://icml.cc/Conferences/2025">ICML 2025</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-02:</b> 4 papers are accepted to <a href="https://cvpr.thecvf.com/">CVPR 2025</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-01:</b> 1 paper is accepted to <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-07:</b> 4 papers are accepted to <a href="https://eccv.ecva.net/Conferences/2024">ECCV 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-02:</b> 2 papers are accepted to <a href="https://cvpr2024.thecvf.com/">CVPR 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-01:</b> 1 paper is accepted to <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a>.
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
      <td style="text-indent:20px;width:100%;vertical-align:middle">
      <p>
        *Equal contribution &nbsp;&nbsp; <sup>â€ </sup>Project leader/Corresponding author.
      </p>
      </td>
    </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="text-indent:20px;width:100%;vertical-align:middle">
          <p><heading>Newest Papers</heading></p>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/StreamVGGT.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Streaming 4D Visual Geometry Transformer</papertitle>
            <br>  
              <a href="">Dong Zhuo*</a>, 
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="">Jiahe Guo</a>, 
              <a href="https://github.com/YkiWu">Yuqi Wu</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
            <em><strong>arXiv</strong></em>, 2025.
            <br>
            <a href="https://arxiv.org/abs/2507.11539">[arXiv]</a> 
            <a href="https://github.com/wzzheng/StreamVGGT">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=StreamVGGT&type=star&count=true" >
            </iframe>
            <a href="https://wzzheng.net/StreamVGGT/">[Project Page]</a>
            <!-- <a href="https://zhuanlan.zhihu.com/p/12867656844">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
            <br>
            <p> StreamVGGT employs temporal causal attention and leverages cached token memory to support efficient incremental on-the-fly reconstruction, enabling interative and real-time online applications.  </p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/Point3R.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory</papertitle>
            <br> 
              <a href="https://github.com/YkiWu">Yuqi Wu*</a>,  
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
            <em><strong>arXiv</strong></em>, 2025.
            <br>
            <a href="https://arxiv.org/abs/2507.02863">[arXiv]</a> 
            <a href="https://github.com/YkiWu/Point3R">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=YkiWu&repo=Point3R&type=star&count=true" >
            </iframe>
            <a href="https://ykiwu.github.io/Point3R/">[Project Page]</a>
            <!-- <a href="https://zhuanlan.zhihu.com/p/12867656844">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
            <br>
            <p> Point3R is an online framework for dense streaming 3D reconstruction using explicit spatial memory, which achieves competitive performance with low training costs.  </p>
          </td>
        </tr>



      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="images/GenWorld.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>GenWorld: Towards Detecting AI-generated Real-world Simulation Videos</papertitle>
          <br> 
              <a href="https://chen-wl20.github.io/"> Weiliang Chen</a>,
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href="https://yzheng97.github.io/">Yu Zheng</a>,
              <a href="https://leichenthu.github.io/">Lei Chen</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://duanyueqi.github.io/">Yueqi Duan</a>
          <br>
          <em><strong>arXiv</strong></em>, 2025.
          <br>
          <a href="https://arxiv.org/abs/2506.10975">[arXiv]</a> 
          <a href="https://github.com/chen-wl20/GenWorld">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=chen-wl20&repo=GenWorld&type=star&count=true" >
          </iframe>
          <a href="https://chen-wl20.github.io/GenWorld/">[Project Page]</a>
          <!-- <a href="https://mp.weixin.qq.com/s?__biz=MzU4NjQ1NzQyNQ==&mid=2247486813&idx=1&sn=a17268e992e758b6b163c2d94eed6cb5&chksm=fcd6a19ab897918046a1f0b997d312e8a8d01ad9aafa38615ecefdf2f68cfb95a267b8f0ca25&mpshare=1&scene=1&srcid=0131JRQQdlxwyTP7HDo4d0BT&sharer_shareinfo=310108161e2f4bda7291cfbbff104f1d&sharer_shareinfo_first=6b27789183ea7efa584ca5ae176dfe55#rd">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
          <br>
          <p> GenWorld features three key characteristics: 1) Real-world Simulation, 2) High Quality, and 3) Cross-prompt Diversity, which can serve as a foundation for AI-generated video detection research with practical significance.</p>
        </td>
      </tr>


      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="images/QuadricFormer.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>QuadricFormer: Scene as Superquadrics for 3D Semantic Occupancy Prediction</papertitle>
          <br>  
            <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo*</a>,
            <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
            <a href="">Xiaoyong Han*</a>,
            <a href="">Longchao Yang</a>,
            <a href="">Yong Pan</a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
          <br>
          <em><strong>arXiv</strong></em>, 2025.
          <br>
          <a href="https://arxiv.org/abs/2506.10977">[arXiv]</a> 
          <a href="https://github.com/zuosc19/QuadricFormer">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=zuosc19&repo=QuadricFormer&type=star&count=true" >
          </iframe>
          <!-- <a href="https://wzzheng.net/OccWorld/">[Project Page]</a> -->
          <!-- <a href="https://zhuanlan.zhihu.com/p/16608177027">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
          <br>
          <p> QuadricFormer proposes geometrically expressive superquadrics as scene primitives, enabling efficient and powerful object-centric representation of driving scenes.</p>
        </td>
      </tr>


      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="images/SceneCompleter.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis</papertitle>
          <br> 
              <a href="https://chen-wl20.github.io/"> Weiliang Chen</a>,
              <a href="">Jiayi Bi</a>,
              <a href="https://huang-yh.github.io/"> Yuanhui Huang</a>, 
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href="https://duanyueqi.github.io/">Yueqi Duan</a>
          <br>
          <em><strong>arXiv</strong></em>, 2025.
          <br>
          <a href="https://arxiv.org/abs/2506.10981">[arXiv]</a> 
          <a href="https://github.com/chen-wl20/SceneCompleter">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=chen-wl20&repo=SceneCompleter&type=star&count=true" >
          </iframe>
          <a href="https://chen-wl20.github.io/SceneCompleter/">[Project Page]</a>
          <!-- <a href="https://mp.weixin.qq.com/s?__biz=MzU4NjQ1NzQyNQ==&mid=2247486813&idx=1&sn=a17268e992e758b6b163c2d94eed6cb5&chksm=fcd6a19ab897918046a1f0b997d312e8a8d01ad9aafa38615ecefdf2f68cfb95a267b8f0ca25&mpshare=1&scene=1&srcid=0131JRQQdlxwyTP7HDo4d0BT&sharer_shareinfo=310108161e2f4bda7291cfbbff104f1d&sharer_shareinfo_first=6b27789183ea7efa584ca5ae176dfe55#rd">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
          <br>
          <p> SceneCompleter explores 3D scene completion for generative novel view synthesis by jointly modeling geometry and appearance.</p>
        </td>
      </tr>


    </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="text-indent:20px;width:100%;vertical-align:middle">
              <p><heading>Selected Papers <a href="https://scholar.google.com/citations?hl=zh-CN&user=LdK9scgAAAAJ&view_op=list_works&sortby=pubdate"> [Full List] </a></heading></p>
              <!-- <p>
                *Equal contribution &nbsp;&nbsp; <sup>â€ </sup>Project leader/Corresponding author.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          

          <h3 style="text-indent:20px;color:brown">World Model</h3>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Owl.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Owl-1: Omni World Model for Consistent Long Video Generation</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href=""> Yuan Gao</a>,  
              <a href=""> Xin Tao</a>,  
              <a href=""> Pengfei Wan</a>,  
              <a href=""> Di Zhang</a>,  
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em><strong>arXiv</strong></em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2412.09600">[arXiv]</a> 
              <a href="https://github.com/huang-yh/Owl">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=huang-yh&repo=Owl&type=star&count=true">
              </iframe>
              <!-- <a href="https://wzzheng.net/OccWorld/">[Project Page]</a> -->
              <a href="https://www.jiqizhixin.com/articles/2025-01-15">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> Owl-1 approaches consistent long video generation with an omni world model, which models the evolution of the underlying world with latent state, explicit observation and world dynamics variables.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GeoDrive.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control</papertitle>
              <br> 
              <a href="https://antonioo-c.github.io/">Anthony Chen*</a>, 
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://wangyida.github.io/">Yida Wang*</a>,  
              <a href="">Xueyang Zhang</a>,  
              <a href="">Kun Zhan</a>,  
              <a href="">Peng Jia</a>,  
              <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer</a>,
              <a href="https://www.shanghangzhang.com/"> Shanghang Zhang</a>
              <br>
              <em><strong>arXiv</strong></em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2505.22421">[arXiv]</a> 
              <a href="https://github.com/antonioo-c/GeoDrive">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=antonioo-c&repo=GeoDrive&type=star&count=true">
              </iframe>
              <a href="https://www.youtube.com/watch?v=LECkvCff6v0&ab_channel=aifreshboy">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/1915939289679757511">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> GeoDrive integrates robust 3D conditions into driving world models, enhancing spatial understanding and action controllability.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GaussianWorld.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction</papertitle>
              <br>  
              <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ">Sicheng Zuo*</a>,
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ">Yuanhui Huang</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2412.10373">[arXiv]</a> 
              <a href="https://github.com/zuosc19/GaussianWorld">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=zuosc19&repo=GaussianWorld&type=star&count=true" >
              </iframe>
              <!-- <a href="https://wzzheng.net/OccWorld/">[Project Page]</a> -->
              <a href="https://zhuanlan.zhihu.com/p/17968801773">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> GaussianWorld reformulates 3D occupancy prediction as a 4D occupancy forecasting problem conditioned on the current sensor input and propose a Gaussian World Model to exploit the scene evolution for perception.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/OccWorld.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving</papertitle>
              <br> 
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://github.com/chen-wl20">Weiliang Chen*</a>,  
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ">Yuanhui Huang</a>, 
              <a href="http://boruizhang.site/">Borui Zhang</a>, 
              <a href="https://duanyueqi.github.io/">Yueqi Duan</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong></em>), 2024.
              <br>
              <a href="https://arxiv.org/abs/2311.16038">[arXiv]</a> 
              <a href="https://github.com/wzzheng/OccWorld">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=OccWorld&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/OccWorld/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/669979822">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> OccWorld models the joint evolutions of 3D scenes and ego movements and paves the way for interpretable end-to-end large driving models.  </p>
            </td>
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <h3 style="text-indent:20px;color:brown">Efficient Large Model</h3>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SparseVLM.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference</papertitle>
              <br> 
              <a href="https://gumpest.github.io/">Yuan Zhang*</a>,  
              <a href="https://scholar.google.com/citations?user=TxeAbWkAAAAJ">Chun-Kai Fan*</a>,  
              <a href="">Junpeng Ma*</a>,  
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href="https://taohuang.info/">Tao Huang</a>,  
              <a href="https://cfcs.pku.edu.cn/people/faculty/kuancheng/index.htm">Kuan Cheng</a>,  
              <a href="https://gudovskiy.github.io/">Denis Gudovskiy</a>,  
              <a href="">Tomoyuki Okuno</a>,  
              <a href="">Yohei Nakata</a>,  
              <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer</a>,
              <a href="https://www.shanghangzhang.com/"> Shanghang Zhang</a>
              <br>
              <em>International Conference on Machine Learning (<strong>ICML</strong></em>), 2025.
              <br>
              <a href="https://arxiv.org/abs/2410.04417">[arXiv]</a> 
              <a href="https://github.com/Gumpest/SparseVLMs">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=Gumpest&repo=SparseVLMs&type=star&count=true" >
              </iframe>
              <a href="https://leofan90.github.io/SparseVLMs.github.io/">[Project Page]</a>
              <!-- <a href="https://www.jiqizhixin.com/articles/2025-01-15">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
              <br>
              <p>SparseVLM sparsifies visual tokens adaptively based on the question prompt.</p>
            </td>
          </tr>



        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:brown">Vision Foundation Model</h3>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SegAnyMo.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Segment Any Motion in Videos</papertitle>
              <br>  
              <a href="https://github.com/nnanhuang"> Nan Huang</a>,
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://www.chenfengx.com/"> Chenfeng Xu </a>, 
              <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>, 
              <a href="https://www.shanghangzhang.com/"> Shanghang Zhang</a>,
              <a href="https://people.eecs.berkeley.edu/~kanazawa/"> Angjoo Kanazawa</a>,
              <a href="https://qianqianwang68.github.io/"> Qianqian Wang</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2503.22268">[arXiv]</a> 
              <a href="https://github.com/nnanhuang/SegAnyMo">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=nnanhuang&repo=SegAnyMo&type=star&count=true" >
              </iframe>
              <a href="https://motion-seg.github.io/">[Project Page]</a>
              <!-- <a href="https://zhuanlan.zhihu.com/p/17968801773">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
              <br>
              <p> Our model produces instance-level fine-grained moving object masks and can handle challenging scenarios including articulated structures, shadow reflections, dynamic background motion, and drastic camera movement.</p>
            </td>
          </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/GlobalMamba.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>GlobalMamba: Global Image Serialization for Vision Mamba</papertitle>
            <br>
            <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ">Chengkun Wang*</a>,
            <strong>Wenzhao Zheng*<sup>â€ </sup></strong>,
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>,
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
            <em><strong>arXiv</strong></em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2410.10316">[arXiv]</a>
            <a href="https://github.com/wangck20/GlobalMamba">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=wangck20&repo=GlobalMamba&type=star&count=true" >
            </iframe>
            <br>
            <p> GlobalMamba constructs a causal token sequence by frequency, while ensuring that tokens acquire global feature information.</p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/V2M.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>V2M: Visual 2-Dimensional Mamba for Image Representation Learning</papertitle>
            <br>
            <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ">Chengkun Wang*</a>,
            <strong>Wenzhao Zheng*<sup>â€ </sup></strong>,
            <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ">Yuanhui Huang</a>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>,
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
            <em><strong>arXiv</strong></em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2410.10382">[arXiv]</a>
            <a href="https://github.com/wangck20/V2M">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=wangck20&repo=V2M&type=star&count=true" >
            </iframe>
            <br>
            <p>Visual 2-Dimensional Mamba (V2M) generalize SSM to the 2-dimensional space and generates the next state considering two adjacent states on both dimensions (e.g., columns and rows) which directly processes image tokens in the 2D space.</p>
          </td>
        </tr>
        
        
        </tbody></table>
        


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <h3 style="text-indent:20px;color:brown">Image Generation</h3>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SpectralAR.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SpectralAR: Spectral Autoregressive Visual Generation</papertitle>
              <br> 
              <a href="https://huang-yh.github.io/"> Yuanhui Huang</a>, 
              <a href="https://chen-wl20.github.io/"> Weiliang Chen</a>,
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href="https://duanyueqi.github.io/">Yueqi Duan</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2506.10962">[arXiv]</a> 
              <a href="https://github.com/huang-yh/SpectralAR">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=huang-yh&repo=SpectralAR&type=star&count=true" >
              </iframe>
              <a href="https://huang-yh.github.io/spectralar/">[Project Page]</a>
              <!-- <a href="https://mp.weixin.qq.com/s?__biz=MzU4NjQ1NzQyNQ==&mid=2247486813&idx=1&sn=a17268e992e758b6b163c2d94eed6cb5&chksm=fcd6a19ab897918046a1f0b997d312e8a8d01ad9aafa38615ecefdf2f68cfb95a267b8f0ca25&mpshare=1&scene=1&srcid=0131JRQQdlxwyTP7HDo4d0BT&sharer_shareinfo=310108161e2f4bda7291cfbbff104f1d&sharer_shareinfo_first=6b27789183ea7efa584ca5ae176dfe55#rd">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
              <br>
              <p> We propose a Spectral AutoRegressive (SpectralAR) visual generation framework to achieve causality for visual sequences from the spectral perspective.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GaussianToken.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting</papertitle>
              <br> 
              <a href=""> Jiajun Dong*</a>, 
              <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ"> Chengkun Wang*</a>,
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href="https://leichenthu.github.io/">Lei Chen</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>,
              <a href="https://andytang15.github.io/"> Yansong Tang</a>
              <br>
              <em><strong>arXiv</strong></em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2501.15619">[arXiv]</a> 
              <a href="https://github.com/ChrisDong-THU/GaussianToken">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=ChrisDong-THU&repo=GaussianToken&type=star&count=true" >
              </iframe>
              <!-- <a href="https://wzzheng.net/OccWorld/">[Project Page]</a> -->
              <a href="https://mp.weixin.qq.com/s?__biz=MzU4NjQ1NzQyNQ==&mid=2247486813&idx=1&sn=a17268e992e758b6b163c2d94eed6cb5&chksm=fcd6a19ab897918046a1f0b997d312e8a8d01ad9aafa38615ecefdf2f68cfb95a267b8f0ca25&mpshare=1&scene=1&srcid=0131JRQQdlxwyTP7HDo4d0BT&sharer_shareinfo=310108161e2f4bda7291cfbbff104f1d&sharer_shareinfo_first=6b27789183ea7efa584ca5ae176dfe55#rd">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> GaussianToken represents each image by a set of 2D Gaussian tokens in a feed-forward manner.</p>
            </td>
          </tr>



        </tbody></table>



      


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
      
        <h3 style="text-indent:20px;color:green">Large Driving Model<a href="https://github.com/wzzheng/LDM"> [Page]</a></h3>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/Doe-1.gif" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Doe-1: Closed-Loop Autonomous Driving with Large World Model</papertitle>
            <br> 
            <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
            <a href="">Zetian Xia*</a>,  
            <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
            <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo</a>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
            <em><strong>arXiv</strong></em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2412.09627">[arXiv]</a> 
            <a href="https://github.com/wzzheng/Doe">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=Doe&type=star&count=true" >
            </iframe>
            <a href="https://wzzheng.net/Doe/">[Project Page]</a>
            <a href="https://zhuanlan.zhihu.com/p/12867656844">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
            <br>
            <p> Doe-1 is the first closed-loop autonomous driving model for unified perception, prediction, and planning.  </p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/GPD-1.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>GPD-1: Generative Pre-training for Driving</papertitle>
            <br> 
            <a href="">Zetian Xia*</a>,  
            <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo*</a>, 
            <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
            <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang</a>, 
            <a href=""> Dalong Du</a>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou</a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>,
            <a href="https://www.shanghangzhang.com/"> Shanghang Zhang</a>
            <br>
            <em><strong>arXiv</strong></em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2412.08643">[arXiv]</a> 
            <a href="https://github.com/wzzheng/GPD">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=GPD&type=star&count=true" >
            </iframe>
            <a href="https://wzzheng.net/GPD/">[Project Page]</a>
            <a href="https://zhuanlan.zhihu.com/p/17035347215">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
            <br>
            <p> GPD-1 proposes a unified approach that seamlessly accomplishes multiple aspects of scene evolution, including scene simulation, traffic simulation, closed-loop simulation, map prediction, and motion planning, all without additional fine-tuning.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="./images/Stag.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model</papertitle>
            <br> 
            <a href="https://github.com/LeningWang"> Lening Wang* </a>, 
            <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
            <a href=""> Dalong Du</a>, 
            <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang</a>, 
            <a href="https://shi.buaa.edu.cn/renyilong/zh_CN/index.htm"> Yilong Ren </a>, 
            <a href="https://scholar.google.com/citations?user=d0WJTQgAAAAJ&amp;hl"> Han Jiang </a>, 
            <a href="https://zhiyongcui.com/"> Zhiyong Cui </a>, 
            <a href="https://shi.buaa.edu.cn/09558/zh_CN/index.htm"> Haiyang Yu </a>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou</a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>,
            <a href="https://www.shanghangzhang.com/"> Shanghang Zhang</a>
            <br>
            <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025.
            <br>
            <a href="https://arxiv.org/abs/2412.05280">[arXiv]</a> 
            <a href="https://github.com/wzzheng/Stag">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=Stag&type=star&count=true" >
            </iframe>
            <a href="https://wzzheng.net/Stag/">[Project Page]</a>
            <br>
            <p> Spatial-Temporal simulAtion for drivinG (Stag-1) enables controllable 4D autonomous driving simulation with spatial-temporal decoupling.  </p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="./images/Driv3R.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Driv3R: Learning Dense 4D Reconstruction for Autonomous Driving</papertitle>
            <br> 
            <a href="https://scholar.google.com/citations?user=r9rsD_0AAAAJ&hl=zh-CN&oi=sra"> Xin Fei </a>, 
            <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
            <a href="https://duanyueqi.github.io/">Yueqi Duan</a>,
            <a href="https://zhanwei.site/"> Wei Zhan </a>,
            <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/"> Masayoshi Tomizuka </a>,
            <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>,
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>
            <br>
            <em><strong>arXiv</strong></em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2412.06777">[arXiv]</a> 
            <a href="https://github.com/Barrybarry-Smith/Driv3R">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=Barrybarry-Smith&repo=Driv3R&type=star&count=true" >
            </iframe>
            <a href="https://wzzheng.net/Driv3R/">[Project Page]</a>
            <br>
            <p> Driv3R predicts per-frame pointmaps in the global consistent coordinate system in an optimization-free manner. </p>
          </td>
        </tr>

      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="./images/UniDrive.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>UniDrive: Towards Universal Driving Perception Across Camera Configurations</papertitle>
          <br> 
          <a href="https://github.com/LeningWang"> Ye Li</a>, 
          <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
          <a href="https://robotics.umich.edu/profile/xiaonan-sean-huang/"> Xiaonan Huang </a>,
          <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>
          <br>
          <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025.
          <br>
          <a href="https://arxiv.org/abs/2410.13864">[arXiv]</a> 
          <a href="https://github.com/ywyeli/UniDrive">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=ywyeli&repo=UniDrive&type=star&count=true" >
          </iframe>
          <a href="https://wzzheng.net/UniDrive/">[Project Page]</a>
          <a href="https://zhuanlan.zhihu.com/p/3904342108">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
          <br>
          <p> UniDrive presents the first comprehensive framework designed to generalize vision-centric 3D perception models across diverse camera configurations. </p>
        </td>
      </tr>  


      </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          
          
            <h3 style="text-indent:20px;color:green">End-to-End Autonomou Driving</h3>

            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/GaussianAD.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <papertitle>GaussianAD: Gaussian-Centric End-to-End Autonomous Driving</papertitle>
                <br>  
                <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
                <a href=""> Junjie Wu*</a>,
                <a href=""> Yao Zheng*</a>,
                <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo</a>,
                <a href=""> Zixun Xie</a>,
                <a href=""> Longchao Yang</a>,
                <a href=""> Yong Pan</a>,
                <a href=""> Zhihui Hao</a>,
                <a href=""> Peng Jia</a>,
                <a href=""> Xianpeng Lang</a>,
                <a href="https://www.shanghangzhang.com/"> Shanghang Zhang</a>
                <br>
                <em><strong>arXiv</strong></em>, 2024.
                <br>
                <a href="https://arxiv.org/abs/2412.10371">[arXiv]</a> 
                <a href="https://github.com/wzzheng/GaussianAD">[Code]</a>
                <iframe
                style="margin-left: 2px; margin-bottom:-5px;"
                frameborder="0" scrolling="0" width="91px" height="20px"
                src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=GaussianAD&type=star&count=true" >
                </iframe>
                <!-- <a href="https://wzzheng.net/OccWorld/">[Project Page]</a> -->
                <a href="https://zhuanlan.zhihu.com/p/16608177027">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
                <br>
                <p> GaussianAD is a Gaussian-centric end-to-end framework which employs sparse yet comprehensive 3D Gaussians to pass information through the pipeline to efficiently preserve more details.</p>
              </td>
            </tr>
          

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GenAD.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GenAD: Generative End-to-End Autonomous Driving</papertitle>
              <br> 
              <strong>Wenzhao Zheng*</strong>, 
              <a href="https://github.com/songruiqi"> Ruiqi Song* </a>,  
              <a href="https://scholar.google.com/citations?user=jPvOqgYAAAAJ"> Xianda Guo* </a>, 
              <a href=""> Chenming Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=jzvXnkcAAAAJ"> Long Chen</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong></em>), 2024.
              <br>
              <a href="https://arxiv.org/abs/2402.11502">[arXiv]</a> 
              <a href="https://github.com/wzzheng/GenAD">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=GenAD&type=star&count=true" >
              </iframe>
              <a href="https://zhuanlan.zhihu.com/p/683302211">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> GenAD casts end-to-end autonomous driving as a generative modeling problem.  </p>
            </td>
          </tr>


        </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          
            <h3 style="text-indent:20px;color:green">3D Occupancy Prediction</h3>

            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/GaussianFormer-2.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <papertitle>GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction</papertitle>
                <br> 
                <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ">Yuanhui Huang</a>, 
                <a href="">Amonnut Thammatadatrakoon</a>, 
                <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
                <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang</a>, 
                <a href="">Dalong Du</a>, 
                <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025.
                <br>
                <a href="https://arxiv.org/abs/2412.04384">[arXiv]</a> 
                <a href="https://github.com/huang-yh/GaussianFormer">[Code]</a>
                <iframe
                style="margin-left: 2px; margin-bottom:-5px;"
                frameborder="0" scrolling="0" width="91px" height="20px"
                src="https://ghbtns.com/github-btn.html?user=huang-yh&repo=GaussianFormer&type=star&count=true" >
                </iframe>
                <a href="https://wzzheng.net/GaussianFormer/">[Project Page]</a>
                <!-- <a href="https://zhuanlan.zhihu.com/p/700833107">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
                <br>
                <p> GaussianFormer-2 interprets each Gaussian as a probability distribution of its neighborhood being occupied and conforms to probabilistic multiplication to derive the overall geometry.</p>
              </td>
            </tr>  


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GaussianFormer.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong></em>), 2024.
              <br>
              <a href="https://arxiv.org/abs/2405.17429">[arXiv]</a> 
              <a href="https://github.com/huang-yh/GaussianFormer">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=huang-yh&repo=GaussianFormer&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/GaussianFormer/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/700833107">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> GaussianFormer proposes the 3D semantic Gaussians as <b>a more efficient object-centric</b> representation for driving scenes compared with 3D occupancy.  </p>
            </td>
          </tr>     

          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SelfOcc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang* </a>, 
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="http://boruizhang.site/"> Borui Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2311.12754">[arXiv]</a>
              <a href="https://github.com/huang-yh/SelfOcc">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=huang-yh&repo=SelfOcc&type=star&count=true" >
              </iframe>
              <a href="https://huang-yh.github.io/SelfOcc/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/677380563">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> SelfOcc is the first self-supervised work that produces reasonable 3D occupancy for surround cameras. </p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/tpvformer.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang* </a>, 
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2302.07817">[arXiv]</a>
              <a href="https://github.com/wzzheng/TPVFormer">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="100px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=TPVFormer&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/TPVFormer/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/614984007">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> Given only surround-camera motorcycle RGB images barrier as inputs, our model (trained using trailer only sparse traffic cone LiDAR point supervision) can predict the semantic occupancy for all volumes in the 3D space. </p>
            </td>
          </tr>



          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:green">3D Scene Reconstruction</h3>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/EmbodiedOcc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding</papertitle>
              <br> 
              <a href="https://github.com/YkiWu">Yuqi Wu*</a>,  
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo</a>, 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2412.04380">[arXiv]</a> 
              <a href="https://github.com/YkiWu/EmbodiedOcc">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=YkiWu&repo=EmbodiedOcc&type=star&count=true" >
              </iframe>
              <a href="https://ykiwu.github.io/EmbodiedOcc/">[Project Page]</a>
              <!-- <a href="https://zhuanlan.zhihu.com/p/12867656844">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
              <br>
              <p> EmbodiedOcc formulates an embodied 3D occupancy prediction task and employs a Gaussian-based framework to accomplish it.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./images/PixelGaussian.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?user=r9rsD_0AAAAJ&hl=zh-CN&oi=sra"> Xin Fei </a>, 
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href="https://duanyueqi.github.io/">Yueqi Duan</a>,
              <a href="https://zhanwei.site/"> Wei Zhan </a>,
              <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/"> Masayoshi Tomizuka </a>,
              <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>
              <br>
              <em><strong>arXiv</strong></em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2410.18979">[arXiv]</a> 
              <a href="https://github.com/Barrybarry-Smith/PixelGaussian">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=Barrybarry-Smith&repo=PixelGaussian&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/PixelGaussian/">[Project Page]</a>
              <br>
              <p> PixelGaussian dynamically adjusts the Gaussian distributions based on geometric complexity in a feed-forward framework. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/S3Gaussian.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>S<sup>3</sup>Gaussian: Self-Supervised Street Gaussians for Autonomous Driving</papertitle>
              <br> 
              <a href="https://github.com/nnanhuang"> Nan Huang </a>, 
              <a href="https://ucwxb.github.io/"> Xiaobao Wei </a>, 
              <strong>Wenzhao Zheng<sup>â€ </sup></strong>, 
              <a href=""> Pengju An </a>, 
              <a href="https://lu-m13.github.io/"> Ming Lu </a>, 
              <a href="https://zhanwei.site/"> Wei Zhan </a>, 
              <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/"> Masayoshi Tomizuka </a>, 
              <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>, 
              <a href="https://www.shanghangzhang.com/"> Shanghang Zhang </a>
              <br>
              <em><strong>arXiv</strong></em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2405.20323">[arXiv]</a> 
              <a href="https://github.com/nnanhuang/S3Gaussian">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=nnanhuang&repo=S3Gaussian&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/S3Gaussian/">[Project Page]</a>
              <br>
              <p> S<sup>3</sup>Gaussian employs 3D Gaussians to model dynamic scenes for autonomous driving <b>without</b> other supervisions (e.g., 3D bounding boxes).  </p>
            </td>
          </tr>


          </tbody></table>


        
          


          <div id="click">
            <p align=right><a href="#Show-the-full-publication-list" style="padding:20px;" onclick="showStuff(this);">Other topics</a></p>
           </div>
           <script>
             function showStuff(txt) {
               document.getElementById("full").style.display = "block";
               document.getElementById("click").style.display = "none";
              //  document.getElementById("selected").style.display = "none";
             }
             </script>

      <div id="full">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="text-indent:20px;width:100%;vertical-align:middle">
            <p><heading>Other Topics</heading></p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:black">Deep Metric Learning (My Ph.D. Research Topic)</h3>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/IDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Introspective Deep Metric Learning</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ"> Chengkun Wang* </a>,
              <strong>Wenzhao Zheng*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2205.04449">[arXiv]</a>
              <a href="https://github.com/wangck20/IDML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=IDML&type=star&count=true" >
              </iframe>
              <br>
              <p> We propose an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/dml-dc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Metric Learning with Adaptively Composite Dynamic Constraints</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2023.
              <br>
              <a href="https://cloud.tsinghua.edu.cn/f/04dfc89e8f684a0ab4a3/?dl=1">[PDF]</a>
              <!-- <a href="https://github.com/wzzheng">[Code] (coming soon)</a> -->
              <br>
              <p> This paper formulates deep metric learning under a unified framework and propose a dynamic constraint generator to produce adaptive composite constraints to train the metric towards good generalization. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/HDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Hardness-Aware Deep Metric Learning</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://apuaachen.github.io/Zhaodong-Chen/"> Zhaodong Chen </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019 <em>(<strong style="color:red;">oral</strong>)</em>.
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2021.
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Hardness-Aware_Deep_Metric_Learning_CVPR_2019_paper.pdf">[PDF]</a>
              <a href="https://ieeexplore.ieee.org/abstract/document/9035438">[PDF] (Journal version)</a>
              <a href="https://github.com/wzzheng/hdml">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=HDML&type=star&count=true" >
              </iframe>
              <br>
              <p> We perform linear interpolation on embeddings to adaptively manipulate their hardness levels and generate corresponding label-preserving synthetics for recycled training.</p>
            </td> 
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DAML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Adversarial Metric Learning</papertitle>
              <br>
              <a href="https://duanyueqi.github.io/">Yueqi Duan</a>,
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://xudonglinthu.github.io/"> Xudong Lin </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018 <em>(<strong style="color:red;">spotlight</strong>)</em>.
              <br>
              <a href="https://duanyueqi.github.io/">Yueqi Duan</a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <strong>Wenzhao Zheng</strong>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>IEEE Transactions on Image Processing (<strong>T-IP</strong>, IF: 11.041)</strong></em>, 2020.
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.pdf">[PDF]</a>
              <a href="https://ieeexplore.ieee.org/abstract/document/8883191">[PDF] (Journal version)</a>
              <a href="https://github.com/anonymous1computervision/DAML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=anonymous1computervision&repo=DAML&type=star&count=true" >
              </iframe>
              <br>
              <p> We generate potential hard negatives adversarial to the learned metric as complements.</p>
            </td> 
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Structural Deep Metric Learning for Room Layout Estimation</papertitle>
              <br>
              <strong>Wenzhao Zheng</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020.
              <br>
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630715.pdf">[PDF]</a>
              <br>
              <p> We are the first to apply deep metric learning to prediction tasks with structured labels.</p>
            </td> 
          </tr>

          
        
      </tbody></table>



      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <h3 style="text-indent:20px;color:black">Visual Representation Learning</h3>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="./images/SpatialFormer.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>SpatialFormer: Towards Generalizable Vision Transformers with Explicit Spatial Understanding</papertitle>
            <br>
            <a href="https://scholar.google.com/citations?user=N-u2i-QAAAAJ"> Han Xiao*</a>, 
            <strong>Wenzhao Zheng*</strong>, 
            <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ">Sicheng Zuo</a>, 
            <a href="https://scholar.google.com/citations?user=_go6DPsAAAAJ">Peng Gao</a>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
            <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024.
            <br>
            <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02019.pdf">[PDF]</a>
            <!-- <a href="https://github.com/Euphoria16/TL-Align">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=nnanhuang&repo=S3Gaussian&type=star&count=true" >
            </iframe> -->
            <br>
            <p> We identify a token fluctuation phenomenon that has suppressed the potential of data mixing strategies for vision transformers. To adress this, we propose a token-label alignment (TL-Align) method to trace the correspondence between transformed tokens and the original tokens to maintain a label for each token. </p>
          </td>
        </tr>
      

      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="./images/OPERA.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions</papertitle>
          <br>
          <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ"> Chengkun Wang* </a>,
          <strong>Wenzhao Zheng*</strong>, 
          <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&amp;hl=zh-CN&amp;oi=sra"> Zheng Zhu</a>, 
          <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&amp;hl=en&amp;authuser=1">Jie Zhou</a>,
          <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
          <br>
          <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023.
          <br>
          <a href="https://arxiv.org/abs/2210.05557">[arXiv]</a>
          <a href="https://github.com/wangck20/OPERA">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=wangck20&repo=OPERA&type=star&count=true" >
          </iframe>
          <br>
          <p> We unify fully supervised and self-supervised contrastive learning and exploit both supervisions from labeled and unlabeled data for training. </p>
        </td>
      </tr>


      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="./images/TL-Align.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>Token-Label Alignment for Vision Transformers</papertitle>
          <br>
          <a href="https://scholar.google.com/citations?user=N-u2i-QAAAAJ&amp;hl=zh-CN&amp;oi=sra">  Han Xiao*</a>, 
          <strong>Wenzhao Zheng*</strong>, 
          <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&amp;hl=zh-CN&amp;oi=sra"> Zheng Zhu</a>, 
          <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&amp;hl=en&amp;authuser=1">Jie Zhou</a>,
          <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
          <br>
          <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023.
          <br>
          <a href="https://arxiv.org/abs/2210.06455">[arXiv]</a>
          <a href="https://github.com/Euphoria16/TL-Align">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=Euphoria16&repo=TL-Align&type=star&count=true" >
          </iframe>
          <br>
          <p> We identify a token fluctuation phenomenon that has suppressed the potential of data mixing strategies for vision transformers. To adress this, we propose a token-label alignment (TL-Align) method to trace the correspondence between transformed tokens and the original tokens to maintain a label for each token. </p>
        </td>
      </tr>

    </tbody></table>



      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <h3 style="text-indent:20px;color:black">Explainable Artificial Intelligence</h3>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/SAMP.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Path Choice Matters for Clear Attribution in Path Methods</papertitle>
            <br>
            <a href="http://boruizhang.site/"> Borui Zhang</a>, 
            <strong>Wenzhao Zheng</strong>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
            <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2401.10442">[arXiv]</a>
            <a href="https://github.com/zbr17/SAMP">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=zbr17&repo=SAMP&type=star&count=true" >
            </iframe>
            <br>
            <p> To address the ambiguity in attributions caused by different path choices, we introduced the Concentration Principle and developed SAMP, an efficient model-agnostic interpreter. By incorporating the infinitesimal constraint (IC) and momentum strategy (MS), SAMP provides superior interpretations.</p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/bort.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Bort: Towards Explainable Neural Networks with Bounded Orthogonal Constraint</papertitle>
            <br>
            <a href="http://boruizhang.site/"> Borui Zhang</a>, 
            <strong>Wenzhao Zheng</strong>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
            <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023.
            <br>
            <a href="https://arxiv.org/abs/2212.09062">[arXiv]</a>
            <a href="https://github.com/zbr17/Bort">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=zbr17&repo=Bort&type=star&count=true" >
            </iframe>
            <br>
            <p> This paper proposes Bort, an optimizer for improving model explainability with boundedness and orthogonality constraints on model parameters, derived from the sufficient conditions of model comprehensibility and transparency.</p>
          </td>
        </tr>



        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/AVSL.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Attributable Visual Similarity Learning</papertitle>
            <br>
            <a href="http://boruizhang.site/"> Borui Zhang</a>, 
            <strong>Wenzhao Zheng</strong>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
            <br>
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022.
            <br>
            <a href="https://arxiv.org/abs/2203.14932">[arXiv]</a>
            <a href="https://github.com/zbr17/DRML">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=zbr17&repo=DRML&type=star&count=true" >
            </iframe>
            <br>
            <p> This paper proposes an attributable visual similarity learning (AVSL) framework, which employs a generalized similarity learning paradigm to represent the similarity between two images with a graph for a more accurate and explainable similarity measure between images.</p>
          </td> 
        </tr>



      </tbody></table>


      </div>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2024 Excellent Doctoral Dissertation of Chinese Association for Artificial Intelligence</li>
                <li style="margin: 5px;"> 2023 Tsinghua Excellent Doctoral Dissertation Award</li>
                <li style="margin: 5px;"> 2023 Beijing Outstanding Graduate</li>
                <li style="margin: 5px;"> 2023 Tsinghua Outstanding Graduate</li>
                <li style="margin: 5px;"> 2022 Xuancheng Scholarship</li>
                <li style="margin: 5px;"> 2021 National Scholarship  (highest scholarship given by the government of China)</li>
                <li style="margin: 5px;"> 2021 CVPR Outstanding Reviewer</li>
                <li style="margin: 5px;"> 2020 Changtong Scholarship  (highest scholarship in the Dept. of Automation)</li>
                <li style="margin: 5px;"> 2019 National Scholarship  (highest scholarship given by the government of China)</li>
                <li style="margin: 5px;"> 2017 Tung OOCL Scholarship</li>
                <li style="margin: 5px;"> 2016 German Scholarship</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / PC Member:</b> CVPR 2019-2025, ICCV 2019-2025, ECCV 2020-2024, NeurIPS 2023-2025, ICLR 2024-2025, ICML 2025, IJCAI 2020-2025, WACV 2020-2025, ICME 2019-2025, 
              </li>
              <li style="margin: 5px;"> 
                <b>Senior PC Member:</b> IJCAI 2021
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> T-PAMI, T-IP, T-MM, T-CSVT, T-NNLS, T-BIOM, T-IST, Pattern Recognition, Pattern Recognition Letters
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <!-- <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=3Xl5HqLv8wQcw477KsV8mPSQEnrm59hQ6peJ0jKbxdw&cl=ffffff&w=a"></script>
	  </div>         -->
	  <br>
	    &copy; Wenzhao Zheng | Last updated: Feb. 1, 2026.
</center></p>
</body>

</html>
